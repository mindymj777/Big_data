{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+\n",
      "| id|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Age|Vehicle_Damage|Annual_Premium|Policy_Sales_Channel|Vintage|Response|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+\n",
      "|  1|  Male| 44|              1|       28.0|                 0|  > 2 Years|           Yes|       40454.0|                26.0|    217|       1|\n",
      "|  2|  Male| 76|              1|        3.0|                 0|   1-2 Year|            No|       33536.0|                26.0|    183|       0|\n",
      "|  3|  Male| 47|              1|       28.0|                 0|  > 2 Years|           Yes|       38294.0|                26.0|     27|       1|\n",
      "|  4|  Male| 21|              1|       11.0|                 1|   < 1 Year|            No|       28619.0|               152.0|    203|       0|\n",
      "|  5|Female| 29|              1|       41.0|                 1|   < 1 Year|            No|       27496.0|               152.0|     39|       0|\n",
      "|  6|Female| 24|              1|       33.0|                 0|   < 1 Year|           Yes|        2630.0|               160.0|    176|       0|\n",
      "|  7|  Male| 23|              1|       11.0|                 0|   < 1 Year|           Yes|       23367.0|               152.0|    249|       0|\n",
      "|  8|Female| 56|              1|       28.0|                 0|   1-2 Year|           Yes|       32031.0|                26.0|     72|       1|\n",
      "|  9|Female| 24|              1|        3.0|                 1|   < 1 Year|            No|       27619.0|               152.0|     28|       0|\n",
      "| 10|Female| 32|              1|        6.0|                 1|   < 1 Year|            No|       28771.0|               152.0|     80|       0|\n",
      "| 11|Female| 47|              1|       35.0|                 0|   1-2 Year|           Yes|       47576.0|               124.0|     46|       1|\n",
      "| 12|Female| 24|              1|       50.0|                 1|   < 1 Year|            No|       48699.0|               152.0|    289|       0|\n",
      "| 13|Female| 41|              1|       15.0|                 1|   1-2 Year|            No|       31409.0|                14.0|    221|       0|\n",
      "| 14|  Male| 76|              1|       28.0|                 0|   1-2 Year|           Yes|       36770.0|                13.0|     15|       0|\n",
      "| 15|  Male| 71|              1|       28.0|                 1|   1-2 Year|            No|       46818.0|                30.0|     58|       0|\n",
      "| 16|  Male| 37|              1|        6.0|                 0|   1-2 Year|           Yes|        2630.0|               156.0|    147|       1|\n",
      "| 17|Female| 25|              1|       45.0|                 0|   < 1 Year|           Yes|       26218.0|               160.0|    256|       0|\n",
      "| 18|Female| 25|              1|       35.0|                 1|   < 1 Year|            No|       46622.0|               152.0|    299|       0|\n",
      "| 19|  Male| 42|              1|       28.0|                 0|   1-2 Year|           Yes|       33667.0|               124.0|    158|       0|\n",
      "| 20|Female| 60|              1|       33.0|                 0|   1-2 Year|           Yes|       32363.0|               124.0|    102|       1|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Driving_License: integer (nullable = true)\n",
      " |-- Region_Code: double (nullable = true)\n",
      " |-- Previously_Insured: integer (nullable = true)\n",
      " |-- Vehicle_Age: string (nullable = true)\n",
      " |-- Vehicle_Damage: string (nullable = true)\n",
      " |-- Annual_Premium: double (nullable = true)\n",
      " |-- Policy_Sales_Channel: double (nullable = true)\n",
      " |-- Vintage: integer (nullable = true)\n",
      " |-- Response: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier, RandomForestClassifier,GBTClassifier\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Term Project\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "csv_file_path = \"/home/hadoop/Downloads/train.csv\" \n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df, train_df = df.randomSplit([0.3, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier, RandomForestClassifier,GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam= 0.01, labelCol=\"Response\")\n",
    "#dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Response\")\n",
    "#rf = RandomForestClassifier(labelCol=\"Response\", featuresCol=\"features\", numTrees=10)\n",
    "#gbt = GBTClassifier(labelCol=\"Response\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "categorical_columns = ['Gender', 'Vehicle_Age','Vehicle_Damage']\n",
    "indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + '_index') for col_name in categorical_columns]\n",
    "\n",
    "feature_columns = [\"Age\", \"Driving_License\", \"Region_Code\", \n",
    "                   \"Previously_Insured\", \"Gender_index\", \"Vehicle_Age_index\", \n",
    "                   \"Vehicle_Damage_index\", \"Annual_Premium\", \n",
    "                   \"Policy_Sales_Channel\", \"Vintage\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "myStages = indexers+[assembler,lr]\n",
    "\n",
    "# Define the pipeline for transformation\n",
    "pipeline = Pipeline(stages=myStages)\n",
    "\n",
    "# Fit and transform the data\n",
    "Model = pipeline.fit(train_df)\n",
    "trainingPred= Model.transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.878245, Precision = 0.771329, Recall = 0.878245, F1 Score = 0.821322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "validationPred = Model.transform(valid_df)\n",
    "# 準確度\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Response\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator_accuracy.evaluate(validationPred)\n",
    "\n",
    "# 加權精確度\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Response\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "precision = evaluator_precision.evaluate(validationPred)\n",
    "\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Response\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "recall = evaluator_recall.evaluate(validationPred)\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Response\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1 = evaluator_f1.evaluate(validationPred)\n",
    "\n",
    "# 合併輸出\n",
    "print(\"Accuracy = %g, Precision = %g, Recall = %g, F1 Score = %g\" % (accuracy, precision,recall,f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將test.csv一筆一筆發送至kafka模擬streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"test_data\"\n",
    "\n",
    "csv_file_path = \"/home/hadoop/Downloads/test.csv\"\n",
    "\n",
    "# Kafka 生產者配置\n",
    "producer_config = {\n",
    "    'bootstrap.servers': kafka_bootstrap_servers,\n",
    "    'client.id': 'python-producer'\n",
    "}\n",
    "\n",
    "# 創建 Kafka 生產者\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# 讀取 CSV 檔案，略過標頭行\n",
    "with open(csv_file_path, newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    \n",
    "    # 略過標頭行\n",
    "    next(csv_reader, None)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        # 將每一行轉換為字串\n",
    "        message_value = ','.join(row)\n",
    "\n",
    "        # 發送消息到 Kafka 主題\n",
    "        producer.produce(kafka_topic, value=message_value.encode('utf-8'), key=None)\n",
    "\n",
    "        # 等待 Kafka 發送消息\n",
    "        producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define the schema for the Kafka message value\n",
    "message_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Driving_License\", IntegerType(), True),\n",
    "    StructField(\"Region_Code\", DoubleType(), True),\n",
    "    StructField(\"Previously_Insured\", IntegerType(), True),\n",
    "    StructField(\"Vehicle_Age\", StringType(), True),\n",
    "    StructField(\"Vehicle_Damage\", StringType(), True),\n",
    "    StructField(\"Annual_Premium\", DoubleType(), True),\n",
    "    StructField(\"Policy_Sales_Channel\", DoubleType(), True),\n",
    "    StructField(\"Vintage\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取kafka的資料並丟進model裡做預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 單筆資料預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------------------------------+\n",
      "|id    |prediction|probability                              |\n",
      "+------+----------+-----------------------------------------+\n",
      "|381110|0.0       |[0.9921324268746067,0.007867573125393323]|\n",
      "+------+----------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "# Kafka consumer configuration\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Replace with your Kafka broker address\n",
    "    'group.id': 'my_consumer_group',        # Replace with your consumer group ID\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer instance\n",
    "consumer = Consumer(consumer_config)\n",
    "\n",
    "# Subscribe to the Kafka topic\n",
    "consumer.subscribe(['test_data'])  # Replace with your Kafka topic\n",
    "\n",
    "# Poll for and print the first message\n",
    "msg = consumer.poll(10.0)  # Timeout set to 10 seconds\n",
    "\n",
    "# Split the comma-separated values\n",
    "values = msg.value().decode('utf-8').split(',')\n",
    "\n",
    "# Map values to the schema\n",
    "row_values = [\n",
    "    int(values[0]),\n",
    "    values[1],\n",
    "    int(values[2]),\n",
    "    int(values[3]),\n",
    "    float(values[4]),\n",
    "    int(values[5]),\n",
    "    values[6],\n",
    "    values[7],\n",
    "    float(values[8]),\n",
    "    float(values[9]),\n",
    "    int(values[10])\n",
    "]\n",
    "\n",
    "# Create a PySpark DataFrame from the row values\n",
    "df = spark.createDataFrame([row_values], schema=message_schema)\n",
    "\n",
    "# Apply the model for prediction\n",
    "prediction_df = Model.transform(df)\n",
    "\n",
    "# Show the predictions (replace with your desired output)\n",
    "prediction_df.select('id','prediction','probability').show(truncate=False)\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 128筆資料預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------------------------------+\n",
      "|id    |prediction|probability                              |\n",
      "+------+----------+-----------------------------------------+\n",
      "|381111|0.0       |[0.6756448130459972,0.3243551869540028]  |\n",
      "|381112|0.0       |[0.7538274162506944,0.24617258374930562] |\n",
      "|381113|0.0       |[0.9572579333713735,0.04274206662862645] |\n",
      "|381114|0.0       |[0.9919025061478679,0.008097493852132098]|\n",
      "|381115|0.0       |[0.9920444588801505,0.00795554111984953] |\n",
      "|381116|0.0       |[0.988666871295324,0.011333128704675954] |\n",
      "|381117|0.0       |[0.9923716775231054,0.0076283224768946]  |\n",
      "|381118|0.0       |[0.7033330150495118,0.29666698495048816] |\n",
      "|381119|0.0       |[0.9914504674319976,0.008549532568002394]|\n",
      "|381120|0.0       |[0.9908595855091915,0.00914041449080849] |\n",
      "|381121|0.0       |[0.8231777872736302,0.17682221272636978] |\n",
      "|381122|0.0       |[0.989762100084346,0.010237899915654047] |\n",
      "|381123|0.0       |[0.7563029356957962,0.2436970643042038]  |\n",
      "|381124|0.0       |[0.7922493590451729,0.2077506409548271]  |\n",
      "|381125|0.0       |[0.9882017482209394,0.011798251779060576]|\n",
      "|381126|0.0       |[0.7862489005187757,0.21375109948122428] |\n",
      "|381127|0.0       |[0.7578513030887565,0.24214869691124352] |\n",
      "|381128|0.0       |[0.9920304636032783,0.00796953639672171] |\n",
      "|381129|0.0       |[0.9931610567116472,0.006838943288352772]|\n",
      "|381130|0.0       |[0.9930805037421213,0.006919496257878688]|\n",
      "+------+----------+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer, KafkaError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Kafka consumer configuration\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # 替換為你的 Kafka broker 位址\n",
    "    'group.id': 'my_consumer_group',        # 替換為你的 consumer group ID\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer instance\n",
    "consumer = Consumer(consumer_config)\n",
    "\n",
    "# Subscribe to the Kafka topic\n",
    "consumer.subscribe(['test_data'])  # 替換為你的 Kafka 主題\n",
    "\n",
    "# Set the number of messages you want to process\n",
    "num_messages_to_process = 128\n",
    "messages_processed = 0\n",
    "\n",
    "result_df = spark.createDataFrame([], schema=message_schema)\n",
    "\n",
    "while messages_processed < num_messages_to_process:\n",
    "    # Poll for messages\n",
    "    msg = consumer.poll(10.0)  # Timeout set to 10 seconds\n",
    "    \n",
    "    if msg is not None:\n",
    "        if not msg.error():\n",
    "            # Decode the message value and split the comma-separated values\n",
    "            values = msg.value().decode('utf-8').split(',')\n",
    "            \n",
    "            # Map values to the schema\n",
    "            row_values = [\n",
    "                int(values[0]),\n",
    "                values[1],\n",
    "                int(values[2]),\n",
    "                int(values[3]),\n",
    "                float(values[4]),\n",
    "                int(values[5]),\n",
    "                values[6],\n",
    "                values[7],\n",
    "                float(values[8]),\n",
    "                float(values[9]),\n",
    "                int(values[10])\n",
    "            ]\n",
    "            \n",
    "            # Create a PySpark DataFrame from the row values\n",
    "            df = spark.createDataFrame([row_values], schema=message_schema)\n",
    "            \n",
    "            # Append the DataFrame to the result DataFrame\n",
    "            result_df = result_df.union(df)\n",
    "            \n",
    "            messages_processed += 1\n",
    "\n",
    "# Apply the model for prediction on the entire result DataFrame\n",
    "prediction_df = Model.transform(result_df)\n",
    "\n",
    "# Show the predictions\n",
    "prediction_df.select('id', 'prediction', 'probability').show(truncate=False)\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將預測結果一筆一筆發送至kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kafka_topic = 'prediction'\n",
    "# 逐筆發送資料到 Kafka\n",
    "for row in prediction_df.select('id', 'prediction', 'probability').collect():\n",
    "    # 將每一行轉換為 CSV 格式的字串\n",
    "    csv_string = \",\".join([str(x) for x in row])\n",
    "    \n",
    "    # 透過 Kafka 生產者發送資料\n",
    "    producer.produce(kafka_topic, value=csv_string)\n",
    "    \n",
    "    # 每筆之間可能需要一些時間間隔，視情況而定\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將新數據也加入training data去建立新的模型，以validation data 當作模擬資料做示範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_topic = 'valid'\n",
    "# 逐筆發送資料到 Kafka\n",
    "for row in valid_df.collect():\n",
    "    # 將每一行轉換為 CSV 格式的字串\n",
    "    csv_string = \",\".join([str(x) for x in row])\n",
    "    \n",
    "    # 透過 Kafka 生產者發送資料\n",
    "    producer.produce(kafka_topic, value=csv_string)\n",
    "    \n",
    "    # 每筆之間可能需要一些時間間隔，視情況而定\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/09 13:51:18 WARN DAGScheduler: Broadcasting large task binary with size 1026.7 KiB\n",
      "24/01/09 13:51:30 WARN DAGScheduler: Broadcasting large task binary with size 1027.9 KiB\n",
      "24/01/09 13:51:30 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:51:42 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:51:42 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:51:45 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:51:45 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:51:47 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:51:47 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:51:49 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:51:50 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:51:52 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:51:52 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:51:54 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:51:54 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:51:57 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:51:57 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:51:59 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:51:59 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:52:01 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:52:01 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:52:03 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "24/01/09 13:52:04 WARN DAGScheduler: Broadcasting large task binary with size 1028.3 KiB\n",
      "24/01/09 13:52:06 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Kafka consumer configuration\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # 替換為你的 Kafka broker 位址\n",
    "    'group.id': 'my_consumer_group',        # 替換為你的 consumer group ID\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer instance\n",
    "consumer = Consumer(consumer_config)\n",
    "\n",
    "# Subscribe to the Kafka topic\n",
    "consumer.subscribe(['valid'])  # Replace with your Kafka topic\n",
    "\n",
    "\n",
    "# Define the schema for the Kafka message value\n",
    "message_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Driving_License\", IntegerType(), True),\n",
    "    StructField(\"Region_Code\", DoubleType(), True),\n",
    "    StructField(\"Previously_Insured\", IntegerType(), True),\n",
    "    StructField(\"Vehicle_Age\", StringType(), True),\n",
    "    StructField(\"Vehicle_Damage\", StringType(), True),\n",
    "    StructField(\"Annual_Premium\", DoubleType(), True),\n",
    "    StructField(\"Policy_Sales_Channel\", DoubleType(), True),\n",
    "    StructField(\"Vintage\", IntegerType(), True),\n",
    "    StructField(\"Response\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Set the number of messages you want to process\n",
    "num_messages_to_process = 128\n",
    "messages_processed = 0\n",
    "\n",
    "while messages_processed < num_messages_to_process:\n",
    "    # Poll for and print the first message\n",
    "    msg = consumer.poll(10.0)  # Timeout set to 10 seconds\n",
    "    \n",
    "    # Split the comma-separated values\n",
    "    values = msg.value().decode('utf-8').split(',')\n",
    "    \n",
    "    # Map values to the schema\n",
    "    row_values = [\n",
    "        int(values[0]),\n",
    "        values[1],\n",
    "        int(values[2]),\n",
    "        int(values[3]),\n",
    "        float(values[4]),\n",
    "        int(values[5]),\n",
    "        values[6],\n",
    "        values[7],\n",
    "        float(values[8]),\n",
    "        float(values[9]),\n",
    "        int(values[10]),\n",
    "        int(values[11])\n",
    "    ]\n",
    "    \n",
    "    # Create a PySpark DataFrame from the row values\n",
    "    df = spark.createDataFrame([row_values], schema=message_schema)\n",
    "    train_df=df.union(train_df)\n",
    "    messages_processed += 1\n",
    "\n",
    "New_Model = pipeline.fit(train_df)\n",
    "trainingPred= New_Model.transform(train_df)\n",
    "\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.878245, Precision = 0.771329, Recall = 0.878245, F1 Score = 0.821322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "validationPred = New_Model.transform(valid_df)\n",
    "# 準確度\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Response\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator_accuracy.evaluate(validationPred)\n",
    "\n",
    "# 加權精確度\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Response\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "precision = evaluator_precision.evaluate(validationPred)\n",
    "\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Response\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "recall = evaluator_recall.evaluate(validationPred)\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Response\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1 = evaluator_f1.evaluate(validationPred)\n",
    "\n",
    "# 合併輸出\n",
    "print(\"Accuracy = %g, Precision = %g, Recall = %g, F1 Score = %g\" % (accuracy, precision,recall,f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
